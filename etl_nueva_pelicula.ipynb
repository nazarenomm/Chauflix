{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acabf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from supabase import create_client\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "# conexión a la base de datos operacional\n",
    "def conectar_operacional():\n",
    "    url_operacional = 'https://ggvtnhsokxrroymxgres.supabase.co'\n",
    "    key_operacional = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdndnRuaHNva3hycm95bXhncmVzIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0ODYzNTgwNCwiZXhwIjoyMDY0MjExODA0fQ.iRMSBrcUSlA-IpiofR6xc4W6_Dq-smhhMs6sBKHk_dA'\n",
    "    supabase_op = create_client(url_operacional, key_operacional)\n",
    "    return supabase_op\n",
    "\n",
    "supabase_op = conectar_operacional()\n",
    "\n",
    "# conexión a la base de datos de MongoDB\n",
    "def conectar_mongo():\n",
    "    uri = 'mongodb+srv://naza:chauflix123@chauflix.g5rhogq.mongodb.net/'\n",
    "    client = MongoClient(uri)\n",
    "    mongo = client['chauflix']\n",
    "    return mongo\n",
    "\n",
    "mongo = conectar_mongo()\n",
    "\n",
    "# conexión al datawarehouse\n",
    "def conectar_datawarehouse():\n",
    "    url_dw = 'https://hcwyzlprqjlwqwdrfrco.supabase.co'\n",
    "    key_dw = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imhjd3l6bHBycWpsd3F3ZHJmcmNvIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0OTA3NjkzOSwiZXhwIjoyMDY0NjUyOTM5fQ.kslrFLfk4e6HRIPs60qjwna4XWiXIPiJSv7988QjLIo'\n",
    "    supabase_dw = create_client(url_dw, key_dw)\n",
    "    return supabase_dw\n",
    "\n",
    "supabase_dw = conectar_datawarehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41259fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from supabase import create_client\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# # conexión a la base de datos operacional\n",
    "# url_operacional = 'https://ggvtnhsokxrroymxgres.supabase.co'\n",
    "# key_operacional = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdndnRuaHNva3hycm95bXhncmVzIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0ODYzNTgwNCwiZXhwIjoyMDY0MjExODA0fQ.iRMSBrcUSlA-IpiofR6xc4W6_Dq-smhhMs6sBKHk_dA'\n",
    "# supabase_op = create_client(url_operacional, key_operacional)\n",
    "\n",
    "# # conexión a la base de datos de MongoDB\n",
    "# uri = 'mongodb+srv://naza:chauflix123@chauflix.g5rhogq.mongodb.net/'\n",
    "# client = MongoClient(uri)\n",
    "# mongo = client['chauflix']\n",
    "\n",
    "# # conexión al datawarehouse\n",
    "# url_dw = 'https://hcwyzlprqjlwqwdrfrco.supabase.co'\n",
    "# key_dw = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imhjd3l6bHBycWpsd3F3ZHJmcmNvIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0OTA3NjkzOSwiZXhwIjoyMDY0NjUyOTM5fQ.kslrFLfk4e6HRIPs60qjwna4XWiXIPiJSv7988QjLIo'\n",
    "# supabase_dw = create_client(url_dw, key_dw)\n",
    "\n",
    "def extraer_tabla_mongo(mongo, nombre_tabla: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae una tabla de la base de datos MongoDB y la guarda en un DataFrame.\n",
    "    \"\"\"\n",
    "    movies = mongo[nombre_tabla].find()\n",
    "    df = pd.DataFrame(movies)\n",
    "    df = df.drop(columns=['_id'])  # Eliminar la columna _id\n",
    "    return df\n",
    "\n",
    "def extraer_tabla_supabase(supabase_op, nombre_tabla: str, cols:str='*') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae una tabla de la base de datos operacional y la guarda en un DataFrame.\n",
    "    \"\"\"\n",
    "    response = supabase_op.table(nombre_tabla).select(cols).execute()\n",
    "    df = pd.DataFrame(response.data)\n",
    "    return df\n",
    "\n",
    "def cargar_tabla_datawarehouse(df: pd.DataFrame, nombre_tabla: str) -> None:\n",
    "    \"\"\"\n",
    "    Carga el DataFrame transformado en la tabla del datawarehouse.\n",
    "    \"\"\"\n",
    "    supabase_dw.schema(\"public\").table(nombre_tabla).insert(df.to_dict(orient='records')).execute()\n",
    "\n",
    "def extraer_tabla_datawarehouse(supabase_dw, nombre_tabla: str, cols:str='*') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae una tabla de la base de datos operacional y la guarda en un DataFrame.\n",
    "    \"\"\"\n",
    "    response = supabase_dw.table(nombre_tabla).select(cols).execute()\n",
    "    df = pd.DataFrame(response.data)\n",
    "    return df\n",
    "\n",
    "#%% ----------dim_movie\n",
    "\n",
    "def agregar_expiration_date(movies_df: pd.DataFrame, licenses_df: pd.DataFrame, licenses_payment_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega la fecha de expiración a las películas en el DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # unir licencias y pagos de licencias\n",
    "    merge = licenses_df.merge(licenses_payment_df, left_on='id', right_on='license_id', how='left')\n",
    "    merge = merge[['movie_id', 'duration', 'date']]\n",
    "\n",
    "    # calcular y agregar fecha de expiración\n",
    "    for row in merge.itertuples():\n",
    "        movie_id = row.movie_id\n",
    "        duration = row.duration\n",
    "        date = row.date\n",
    "\n",
    "        date = pd.to_datetime(date).date()\n",
    "        expiration_date = date + pd.Timedelta(days=duration)\n",
    "        expiration_date = expiration_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        movies_df.loc[movies_df['id'] == movie_id, 'license_expiration'] = expiration_date\n",
    "\n",
    "    return movies_df\n",
    "\n",
    "def validar_si_esta_en_datawarehouse(movies_mongo: pd.DataFrame, movies_dw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Valida si las películas de MongoDB ya están en el datawarehouse.\n",
    "    Retorna un DataFrame con las películas que no están en el datawarehouse.\n",
    "    \"\"\"\n",
    "    df = movies_mongo[~movies_mongo['id'].isin(movies_dw['id'])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "    # Convertir\n",
    "    \n",
    "def calcular_media_rating(ratings_df: pd.DataFrame, movies_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula y agregar la media de ratings\n",
    "    \"\"\"\n",
    "    ratings_df['rating'] = ratings_df['rating']*2  # Convertir a escala de 0 a 10\n",
    "\n",
    "    for row in movies_df.itertuples():\n",
    "        movie_id = row.id\n",
    "        \n",
    "        if not ratings_df[ratings_df['movie_id'] == movie_id]['rating'].empty:\n",
    "            mean_rating = ratings_df[ratings_df['movie_id'] == movie_id]['rating'].mean()\n",
    "            movies_df.loc[movies_df['id'] == movie_id, 'rating'] = mean_rating\n",
    "        \n",
    "        else:\n",
    "            movies_df.loc[movies_df['id'] == movie_id, 'rating'] = 0\n",
    "    \n",
    "    return movies_df\n",
    "\n",
    "def extraer_genero_principal(movies_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae el género principal de las películas.\n",
    "    \"\"\"\n",
    "    for row in movies_df.itertuples():\n",
    "        genres = row.genres\n",
    "        if genres:\n",
    "            primer_genero = genres[0]\n",
    "        else:\n",
    "            primer_genero = \"\"\n",
    "        movies_df.loc[movies_df['id'] == row.id, 'genres'] = primer_genero\n",
    "\n",
    "    return movies_df\n",
    "    \n",
    "def transformar_peliculas(movies_df: pd.DataFrame, ratings_df: pd.DataFrame, licenses_df: pd.DataFrame, licenses_payment_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma el DataFrame de películas para que tenga la estructura adecuada.\n",
    "    \"\"\"\n",
    "    movies_df = movies_df.drop(columns=['release_year', 'duration_min'])\n",
    "\n",
    "    movies_df = agregar_expiration_date(movies_df, licenses_df, licenses_payment_df)\n",
    "\n",
    "    movies_df = calcular_media_rating(ratings_df, movies_df) # calcular la media de ratings por película y agregarla al DataFrame de películas\n",
    "\n",
    "    movies_df = extraer_genero_principal(movies_df) # extraer género principal y reemplazar la lista\n",
    "\n",
    "    movies_df.rename(columns={'genres': 'genre', 'rating': 'mean_rating'}, inplace=True)\n",
    "\n",
    "    return movies_df\n",
    "\n",
    "# # se extraen las tablas necesarias\n",
    "# licenses_df = extraer_tabla_supabase('licenses')\n",
    "# licenses_payment_df = extraer_tabla_supabase('license_payments')\n",
    "# ratings = extraer_tabla_mongo('ratings')\n",
    "# movies = extraer_tabla_mongo('movies')\n",
    "\n",
    "# # se transforma la tabla de películas\n",
    "# movies = transformar_peliculas(movies, ratings, licenses_df, licenses_payment_df)\n",
    "\n",
    "# # se cargan las películas transformadas en el datawarehouse\n",
    "# cargar_tabla_datawarehouse(movies, 'dim_movie')\n",
    "\n",
    "#%% ------------dim_user\n",
    "def agregar_nombre_pais(users_df: pd.DataFrame, countries_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega el nombre del país a los usuarios.\n",
    "    \"\"\"\n",
    "    # join de usuarios y paises\n",
    "    merged_df = pd.merge(users_df, countries_df, left_on='country_id', right_on='id', how='left')\n",
    "    merged_df.drop(columns=['country_id', 'id_y'], inplace=True)\n",
    "\n",
    "    merged_df.rename(columns={'id_x': 'id', 'name': 'country'}, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "    \n",
    "    return users_df\n",
    "def transformar_usuarios(users_df: pd.DataFrame, countries_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    users_df = users_df[['id', 'birth_date', 'country_id']]\n",
    "\n",
    "    users_df = agregar_nombre_pais(users_df, countries_df)\n",
    "\n",
    "    return users_df\n",
    "\n",
    "# users_df = extraer_tabla_supabase('users')\n",
    "# countries_df = extraer_tabla_supabase('country')\n",
    "\n",
    "# users_df = transformar_usuarios(users_df, countries_df)\n",
    "# cargar_tabla_datawarehouse(users_df, 'dim_user')\n",
    "\n",
    "#%% -------------fact_cancelation\n",
    "\n",
    "def obtener_plan_cancelado(subscriptions_df: pd.DataFrame, plans_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtiene el plan cancelado por cada usuario en la última fecha de suscripción.\n",
    "    \"\"\"\n",
    "    # join de suscripciones y planes para obtener el nombre del plan\n",
    "    subscriptions_df.merge(plans_df, left_on='plan_id', right_on='id')\n",
    "\n",
    "    # ordenar por fecha y eliminar duplicados para obtener el último plan de cada usuario\n",
    "    subscriptions_df = subscriptions_df.sort_values(by=['user_id', 'date'], ascending=False)\n",
    "    subscriptions_df = subscriptions_df.drop_duplicates(subset=['user_id'], keep='first')\n",
    "\n",
    "    return subscriptions_df\n",
    "\n",
    "def transformar_cancelaciones(subscriptions_df: pd.DataFrame, plans_df: pd.DataFrame, cancelations_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma el DataFrame de cancelaciones para que tenga la estructura adecuada.\n",
    "    \"\"\"\n",
    "    subscriptions_df = obtener_plan_cancelado(subscriptions_df, plans_df)\n",
    "\n",
    "    # agregamos el plan cancelado a las cancelaciones\n",
    "    cancelations_df = cancelations_df.merge(subscriptions_df, on='user_id', how='left', suffixes=('', '_sub'))\n",
    "    cancelations_df.drop(columns=['plan_id', 'date_sub', 'id_sub'], inplace=True)\n",
    "\n",
    "    return cancelations_df\n",
    "\n",
    "# cancelations_df = extraer_tabla_supabase('cancelations')\n",
    "# plans_df = extraer_tabla_supabase('plans', cols='id, plan_name')\n",
    "# subscriptions_df = extraer_tabla_supabase('subscriptions')\n",
    "\n",
    "# cancelations_df = transformar_cancelaciones(subscriptions_df, plans_df, cancelations_df)\n",
    "# cargar_tabla_datawarehouse(cancelations_df, 'fact_cancelation')\n",
    "\n",
    "#%% -----------fact_subscriptions_payment\n",
    "def agregar_bool_renovation(subscriptions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega una columna booleana que indica si la suscripción es una renovación.\n",
    "    \"\"\"\n",
    "    # ordenar por fecha y eliminar duplicados para obtener la primera suscripción de cada usuario\n",
    "    subscriptions_df[\"renovation\"] = subscriptions_df.sort_values(\"date\").duplicated(subset=[\"user_id\"], keep=\"first\")\n",
    "    subscriptions_df.rename(columns={'id_x': 'id'}, inplace=True)\n",
    "\n",
    "    return subscriptions_df\n",
    "\n",
    "def transformar_suscripciones(subscriptions_df: pd.DataFrame, plans_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma el DataFrame de suscripciones para que tenga la estructura adecuada.\n",
    "    \"\"\"\n",
    "    # join de suscripciones y planes\n",
    "    subscriptions_df = subscriptions_df.merge(plans_df, left_on=\"plan_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "    # agregamos la columna booleana de renovación\n",
    "    subscriptions_df = agregar_bool_renovation(subscriptions_df)\n",
    "\n",
    "    fact_subscription_payment_df = subscriptions_df[[\"id\", \"user_id\", \"date\", \"plan_name\", \"renovation\", \"price\"]] # nos quedamos con las columnas necesarias\n",
    "    fact_subscription_payment_df.rename(columns={'plan_name': 'plan', 'price': 'pricing'}, inplace=True)\n",
    "\n",
    "    return fact_subscription_payment_df\n",
    "\n",
    "# fact_subscription_df = transformar_suscripciones(subscriptions_df, plans_df)\n",
    "# cargar_tabla_datawarehouse(fact_subscription_df, 'fact_subscriptions_payment')\n",
    "\n",
    "#%% ---------------------fact_license_payments\n",
    "\n",
    "def unir_licencias_y_pagos(licenses: list[dict], license_payments: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Une las licencias con los pagos de licencias para obtener la información necesaria.\n",
    "    \"\"\"\n",
    "    fact_license_payment_data = []\n",
    "    for lp in license_payments:\n",
    "        license_info = next((l for l in licenses if l[\"id\"] == lp[\"license_id\"]), None)\n",
    "        if license_info:\n",
    "            fact_license_payment_data.append({\n",
    "                \"id\": lp[\"id\"],\n",
    "                \"price\": license_info[\"price\"],\n",
    "                \"movie_id\": license_info[\"movie_id\"],\n",
    "                \"date\": str(pd.to_datetime(lp[\"date\"]).date()),  # Convertir a string\n",
    "                \"date_next_payment\": str((pd.to_datetime(lp[\"date\"]) + pd.Timedelta(days=license_info[\"duration\"])).date())  # Convertir a string\n",
    "            })\n",
    "    return fact_license_payment_data\n",
    "\n",
    "def filtrar_por_peliculas_en_dw(fact_license_payment_data: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Filtra los pagos de licencias para que solo incluya películas que están en la tabla dim_movie del datawarehouse.\n",
    "    \"\"\"\n",
    "    dim_movies = supabase_dw.table(\"dim_movie\").select(\"*\").execute().data\n",
    "    dim_movie_ids = [dm[\"id\"] for dm in dim_movies]\n",
    "    fact_license_payment_data = [\n",
    "        row for row in fact_license_payment_data if row[\"movie_id\"] in dim_movie_ids\n",
    "    ]\n",
    "    return fact_license_payment_data\n",
    "\n",
    "def transformar_pagos_licencias(licenses: pd.DataFrame, license_payments: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma el DataFrame de pagos de licencias para que tenga la estructura adecuada.\n",
    "    \"\"\"\n",
    "    fact_license_payment_data = unir_licencias_y_pagos(licenses.to_dict(orient='records'), license_payments.to_dict(orient='records'))\n",
    "    \n",
    "    fact_license_payment_data = filtrar_por_peliculas_en_dw(fact_license_payment_data)\n",
    "\n",
    "    return pd.DataFrame(fact_license_payment_data)\n",
    "\n",
    "# licenses = extraer_tabla_supabase('licenses')\n",
    "# license_payments = extraer_tabla_supabase('license_payments')\n",
    "\n",
    "# fact_license_payments_df = transformar_pagos_licencias(licenses, license_payments)\n",
    "# cargar_tabla_datawarehouse(fact_license_payments_df, 'fact_license_payments')\n",
    "\n",
    "#%% ---------------------fact_ratings\n",
    "def convertir_timestamp_a_fecha(ratings_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte una serie de timestamps a fechas.\n",
    "    \"\"\"\n",
    "    ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'])\n",
    "    ratings_df['timestamp'] = ratings_df['timestamp'].dt.date\n",
    "\n",
    "    ratings_df.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "\n",
    "    return ratings_df\n",
    "\n",
    "def transformar_ratings(ratings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma el DataFrame de ratings para que tenga la estructura adecuada.\n",
    "    \"\"\"\n",
    "    ratings_df['rating'] = ratings_df['rating'] * 2  # Convertir a escala de 0 a 10\n",
    "    \n",
    "    ratings_df = convertir_timestamp_a_fecha(ratings_df)\n",
    "\n",
    "    return ratings_df\n",
    "\n",
    "# ratings_df = extraer_tabla_mongo('ratings')\n",
    "# ratings_df = transformar_ratings(ratings_df)\n",
    "# cargar_tabla_datawarehouse(ratings_df, 'fact_ratings')\n",
    "\n",
    "#%% fact_views\n",
    "def transformar_vistas(views_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma el DataFrame de vistas para que tenga la estructura adecuada.\n",
    "    \"\"\"\n",
    "    views_df['watched_at'] = pd.to_datetime(views_df['watched_at'])\n",
    "    views_df = views_df[['user_id', 'watched_at', 'minutes_watched', 'movie_id']]\n",
    "    views_df['watched_at'] = views_df['watched_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
    "\n",
    "    return views_df\n",
    "\n",
    "def cargar_reproducciones_datawarehouse(views_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Carga el DataFrame de reproducciones en la tabla del datawarehouse.\n",
    "    \"\"\"\n",
    "    def pop_batch(df, batch_size): # helper para cargar en tandas\n",
    "        batch = df.iloc[:batch_size].copy()\n",
    "        df.drop(index=df.index[:batch_size], inplace=True)\n",
    "        return batch\n",
    "    \n",
    "    while not views_df.empty:\n",
    "        batch = pop_batch(views_df, 100_000)\n",
    "        cargar_tabla_datawarehouse(batch, 'fact_views')\n",
    "\n",
    "# views_df = extraer_tabla_mongo('views')\n",
    "# views_df = transformar_vistas(views_df)\n",
    "# cargar_reproducciones_datawarehouse(views_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d723ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_peliculas():\n",
    "    \"\"\"\n",
    "    Carga las películas desde MongoDB al datawarehouse.\n",
    "    Esta función extrae las películas de MongoDB, las transforma y las carga en la tabla dim_movie del datawarehouse.\n",
    "    Hace la verificación de si hay nuevas películas para evitar cargas innecesarias.\n",
    "    \"\"\"\n",
    "    # Conectar a las bases de datos\n",
    "    supabase_op = conectar_operacional()\n",
    "    mongo = conectar_mongo()\n",
    "    supabase_dw = conectar_datawarehouse()\n",
    "    \n",
    "    # Extraer datos de las fuentes\n",
    "    movies_mongo = extraer_tabla_mongo(mongo, 'movies')\n",
    "    ratings = extraer_tabla_mongo(mongo, 'ratings')\n",
    "    licenses_df = extraer_tabla_supabase(supabase_op, 'license')\n",
    "    licenses_payment_df = extraer_tabla_supabase(supabase_op, 'license_payment')\n",
    "    movies_dw = extraer_tabla_datawarehouse(supabase_dw, 'dim_movie')\n",
    "\n",
    "    # Identificar películas nuevas (solo las que están en mongo y no en dw)\n",
    "\n",
    "    peliculas_nuevas = validar_si_esta_en_datawarehouse(movies_mongo, movies_dw)\n",
    "    \n",
    "\n",
    "    # Si no hay nuevas películas, no hacer nada\n",
    "    if len(peliculas_nuevas) == 0:\n",
    "        print(\"No se encontraron nuevas películas para registrar.\")\n",
    "        return\n",
    "\n",
    "    # Transformar las nuevas películas al esquema del datawarehouse    \n",
    "    peliculas_nuevas = transformar_peliculas(peliculas_nuevas, ratings, licenses_df, licenses_payment_df)\n",
    "\n",
    "    # Cargar las nuevas películas transformadas al datawarehouse\n",
    "    \n",
    "    cargar_tabla_datawarehouse(peliculas_nuevas, 'dim_movie')\n",
    "    print(f\"Se cargaron {len(peliculas_nuevas)} nuevas películas al datawarehouse.\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4563774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 1 nuevas películas al datawarehouse.\n"
     ]
    }
   ],
   "source": [
    "cargar_peliculas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
